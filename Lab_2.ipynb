{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71561e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1:\n",
      " - file1.txt\n",
      " - file3.txt\n",
      "Cluster 2:\n",
      " - file2.txt\n",
      " - file4.txt\n",
      " - file5.txt\n",
      " - file6.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ameyp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Ensure the necessary NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize and remove stop words\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to implement single-pass clustering\n",
    "def single_pass_clustering(documents, threshold=0.2):\n",
    "    clusters = []  # List of clusters, each cluster is a list of document indices\n",
    "    \n",
    "    # Vectorize the documents using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Convert TF-IDF matrix to dense matrix\n",
    "    tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Process each document one by one\n",
    "    for i, doc_vector in enumerate(tfidf_matrix_dense):\n",
    "        if not clusters:\n",
    "            # If no clusters exist, create the first one\n",
    "            clusters.append([i])\n",
    "        else:\n",
    "            # Calculate the similarity with the centroids of existing clusters\n",
    "            best_cluster = None\n",
    "            max_similarity = 0\n",
    "            \n",
    "            for cluster in clusters:\n",
    "                # Get the centroid of the cluster (mean of the document vectors in the cluster)\n",
    "                cluster_vectors = tfidf_matrix_dense[cluster]\n",
    "                centroid = np.mean(cluster_vectors, axis=0)  # Calculate the centroid\n",
    "                \n",
    "                # Calculate similarity between the document and the centroid\n",
    "                similarity = cosine_similarity([doc_vector], [centroid]).flatten()[0]\n",
    "                \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_cluster = cluster\n",
    "            \n",
    "            # If the maximum similarity exceeds the threshold, add the document to the best cluster\n",
    "            if max_similarity > threshold:\n",
    "                best_cluster.append(i)\n",
    "            else:\n",
    "                # Otherwise, create a new cluster\n",
    "                clusters.append([i])\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Path to the folder containing the text files\n",
    "    folder_path = 'textfiles_2/'\n",
    "    \n",
    "    # Read and preprocess the documents\n",
    "    documents = []\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.txt'):\n",
    "            file_names.append(file_name)\n",
    "            with open(os.path.join(folder_path, file_name), 'r') as file:\n",
    "                text = file.read()\n",
    "                preprocessed_text = preprocess_text(text)\n",
    "                documents.append(preprocessed_text)\n",
    "    \n",
    "    # Apply the single-pass clustering algorithm\n",
    "    clusters = single_pass_clustering(documents, threshold=0.2)\n",
    "    \n",
    "    # Output the clusters\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        print(f\"Cluster {cluster_idx + 1}:\")\n",
    "        for doc_index in cluster:\n",
    "            print(f\" - {file_names[doc_index]}\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02962b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
